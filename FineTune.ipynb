{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "from src.model import D_GAT\n",
    "from src.mol_processing import Read_mol_data, Generate_dataloader\n",
    "from src.train_eval import Train_eval, Test_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to define \n",
    "1. The datset to finetune: dataset\n",
    "2. The task name in file: task_name\n",
    "3. The type of tasks: target_type ('classification', 'regression')\n",
    "4. The metrics to evaluate: metrics ('AUC', 'RMSE', 'MAE')\n",
    "5. The name of stored model: store_name (dataset + '.pth' or None)\n",
    "\n",
    "However, if you test the datasets used in our paper (Tox21 SIDER MUV HIV BBBP BACE ClinTox ToxCast PCBA ESOL FreeSolv Lipo QM7 QM8 QM9), you only need to define the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'QM7' # Tox21 SIDER MUV HIV BBBP BACE ClinTox ToxCast PCBA ESOL FreeSolv Lipo QM7 QM8 QM9\n",
    "config_file_path = './config/config.json'\n",
    "\n",
    "if dataset in ['HIV', 'BBBP', 'Tox21', 'SIDER', 'MUV', 'BACE', 'ClinTox', 'ToxCast', 'PCBA']:\n",
    "    target_type = 'classification'\n",
    "    metrics = 'AUC'\n",
    "    task_name = None\n",
    "elif dataset in ['ESOl', 'Lipo', 'FreeSolv']:\n",
    "    target_type = 'regression'\n",
    "    metrics = 'RMSE'\n",
    "    task_name = None\n",
    "elif dataset in [ 'QM7', 'QM8', 'QM9']:\n",
    "    target_type = 'regression'\n",
    "    metrics = 'MAE'\n",
    "    task_name = None\n",
    "else:\n",
    "    raise RuntimeError('Please define the target type, task_name and metrics!')\n",
    "#     target_type = 'classification'\n",
    "#     metrics = 'AUC'\n",
    "#     target_type = 'regression'\n",
    "#     metrics = 'RMSE'\n",
    "#     metrics = 'MAE'\n",
    "#     task_name = ['u0_atom'] #for QM7\n",
    "\n",
    "store_name = dataset + '.pth'\n",
    "# store_name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is to load and process data, and load pre-training model. Nothing to define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert target_type in ['classification', 'regression']\n",
    "if target_type == 'classification':\n",
    "    assert metrics in ['AUC']\n",
    "elif target_type == 'regression':\n",
    "    assert metrics in ['RMSE', 'MAE']\n",
    "    \n",
    "mol_train, mol_val, mol_test, mean, std = Read_mol_data(dataset, task_name, target_type)\n",
    "train_dataloader, val_dataloader,test_dataloader = Generate_dataloader(dataset, mol_train, mol_val, mol_test)\n",
    "model, best_score = D_GAT(dataset, mol_train, config_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you load the fine-tuned model, next section could be used to evaluate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Following code is to evaluate the fine-tuning model\n",
    "# Loss, auc = Test_NN(dataset, model, test_dataloader, metrics, target_type, mean, std)\n",
    "# print('Mean Loss: ', Loss)\n",
    "# if target_type == 'classification':\n",
    "#     print('Mean AUC: ', auc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to fine-tune your model. It may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To fine-tune the model\n",
    "model, best_score = Train_eval(dataset, model, train_dataloader, val_dataloader,test_dataloader, best_score, config_file_path, store_name, metrics, target_type, mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
